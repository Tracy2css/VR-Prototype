## 20240520 Prototype Details
### Example of Balanced Crossover Design

Suppose there are four experimental conditions: Control (C), Experimental Group 1 (E1), Experimental Group 2 (E2), and Experimental Group 3 (E3). A Latin square can be used to arrange the experiment order:

| Participant | Order 1 | Order 2 | Order 3 | Order 4 |
|-------------|---------|---------|---------|---------|
| 1           | C       | E1      | E2      | E3      |
| 2           | E1      | E2      | E3      | C       |
| 3           | E2      | E3      | C       | E1      |
| 4           | E3      | C       | E1      | E2      |

---
**Original visual settings**: mornitor, keyborads, and a mouse
**Original background sounds**: keyboard, and mouse click
> 
> **Visual Adjustments (VA)**
> - Light Adjustment: Participants adjust brightness and color temperature of individual lighting devices, natural lights through the blinder (sit closed to the window).
> - Material Adjustment: Participants select and modify object materials, including the divider, table, wall, sound box, and desk surface.
> Customized items: calendar, books, photo frames, toys, and vegetations
>
> **Auditory Adjustments (AA)** 
> - Background noise level: adjust the height and material of the divider
> - Background music: activate the sound box, switch across classic music, Jazz, anime...[original sound level will be reduced]
> - Background natural sounds: activate the sound box, switch across bird song, forest ambience, water streams, ocean waves [original sound level will be reduced]
>
> **Thermal Adjustments (TA)** 
> Portable Neck Fans can be used for cooling/warming
---
## Prototype A [from Emotiv to Galea / from Galea to Emotiv]
### Tutorial :
##### VR session: 10 min
guidance for VA + AA + TA in an exisiting virtual enviroonment 

### Game Start [Emotiv to Galea]:
Select positions
move to the prefered postion, "I would like to sit here"
hand controller point to the computer monitor
"confirm"

### C (Control Group: original settings):
***Duration:20min***
##### VR Experience session: 8 min
- Activate Visual and Auditory Items
> **Interaction:** Use gaze tracking or the controller to point at and activate items without any actual interaction adjustments.
> **Observation:** Observe how participants' preferences for changing the same items in the environment vary under different experimental sequences.
- Inquire About Desire to Adjust Design Properties
> **Interaction:** After item activation, a simple interface pops up, and participants make selections using the controller. The selections are multiple choices to adjust design properties for one activated item with gif/video examples.
> **Observation:** Analyze whether participants' preferences for potential design changes differ under different experimental sequences and conditions.

##### VR Cognition performance test: less than 7 min [To(cpt)]
##### Survey in the virtual environment: [To(s)]

### E(a) (Galea + Survey in the real world):
***Duration:20min***
##### VR Experienec session: about 8 min 
- Design adjustment: about 5 min [Ta(v01)]
> VA + AA + TA in the middle sitting position
- Observation and Immersive Experience: 3min
>- After a confirmation of completing the adjustment sessions, participants can stand up and look around the unmodified environment, and enjoy their optimised design outcomes. 
>- Participants are encouraged to sit again and talk their modifications in the immersive experieneces with the headset.
##### VR Cognition performance test: less than 7 min [Ta(cpt)]
##### Survey: less than 8 min [Ta(s)]

### E(b) (Galea + Survey in the virtual world):
##### VR session: about 8 min 
- Design adjustment: about 5 min [Tb(v01)]
> VA + AA + TA in the sitting position closed to the window
- Observation and Immersive Experience: 3min
>- After a confirmation of completing the adjustment sessions, participants can stand up and look around the unmodified environment, and enjoy their optimised design outcomes. 
>- Participants are encouraged to sit again and talk their modifications in the immersive experieneces with the headset.
##### Cognition performance test: less than 7 min [Tb(cpt)]
##### Survey: about 5 min [Tb(s)]

### E(c) (Emotiv + Survey in the virtual world):
### Basic Tutorial in the new headset: 5 min
##### VR session: about 15 min 
- Design adjustment: about 5 min [Tc(v01)]
> VA + AA + TA in the sitting position closed to the window
- Observation and Immersive Experience: 3min
>- After a confirmation of completing the adjustment sessions, participants can stand up and look around the unmodified environment, and enjoy their optimised design outcomes. 
>- Participants are encouraged to sit again and talk their modifications in the immersive experieneces with the headset.
##### Cognition performance test: less than 7 min [Tc(cpt)]
##### Survey: less than 8 min [Tc(s)]
---
## Prototype A: Data collection
#### Subjective Feedback:

#### Objective Feedback
---
## Prototype B

### Tutorial :
##### VR Experience session: 10 min
>VA + AA + TA in an exisiting virtual enviroonment with guidances

### Game Start:
- Select positions
- move to the prefered postion, "I would like to sit here"
- hand controller point to the computer monitor
- "confirm"

### C (Control Group):
##### VR Experience session: 8 min
- Activate Visual and Auditory Items
> **Interaction:** Use gaze tracking or the controller to point at and activate items without any actual interaction adjustments.
> **Observation:** Observe how participants' preferences for changing the same items in the environment vary under different experimental sequences.
- Inquire About Desire to Adjust Design Properties
> **Interaction:** After item activation, a simple interface pops up, and participants make selections using the controller. The selections are multiple choices to adjust design properties for one activated item with gif/video examples.
> **Observation:** Analyze whether participants' preferences for potential design changes differ under different experimental sequences and conditions.

##### VR Cognition performance test: less than 7 min
##### Survey: less than 8 min


### E1 (Visual + Auditory):
##### VR Experience session: 8 min
- Design adjustment: 5 min
> - VA + AA

- Observation and Immersive Experience: 3min
>- After a confirmation of completing the adjustment sessions, participants can stand up and look around the unmodified environment, and enjoy their optimised design outcomes. 
>- Participants are encouraged to sit again and talk their modifications in the immersive experieneces with the headset.
##### VR Cognition performance test: less than 7 min

### E2 (Visual + Thermal):
##### VR Experience session: 8 min
- Design adjustment: 5 min
> - VA + TA

- Observation and Immersive Experience: 3min
>- After a confirmation of completing the adjustment sessions, participants can stand up and look around the unmodified environment, and enjoy their optimised design outcomes. 
>- Participants are encouraged to sit again and talk their modifications in the immersive experieneces with the headset.

##### VR Cognition performance test: less than 7 min
##### Survey: less than 8 min

### E3 (Visual + Auditory + Thermal):
##### VR Experience session: 8 min
- Design adjustment: 5 min
> - VA + AA + TA

- Observation and Immersive Experience: 3min
>- After a confirmation of completing the adjustment sessions, participants can stand up and look around the unmodified environment, and enjoy their optimised design outcomes. 
>- Participants are encouraged to sit again and talk their modifications in the immersive experieneces with the headset.

##### VR Cognition Performance Test: less than 7 min
##### Survey: less than 8 min

---
## Prototype B: Data Collection
#### Subjective Feedback:

#### Objective Feedback:
- Observe Eye Movements and Basic Motion Behavior
To effectively measure the changes in user behavior and comfort in a VR environment, especially during the pre-adjustment, during-adjustment, and post-adjustment phases, you need to directly measure specific data points related to head and eye movements. These measurements will help you calculate meaningful metrics that reflect user behavior and comfort changes.

Here is a breakdown of what data to measure and how to calculate the relevant metrics in English:

### 1. **Head Movement Metrics**

#### Data to Measure

- **Head Position and Orientation**: Track the 3D position (x, y, z coordinates) and the orientation (pitch, yaw, roll angles) of the userâ€™s head over time.

#### Metrics to Calculate

- **Head Rotation Frequency**
  - **Description**: The number of times the head orientation changes direction per unit of time, indicating how often the user is looking around.
  - **Calculation**: Count the number of times the head changes direction significantly (beyond a threshold angle) in a given period and divide by the duration of that period.
  
  ```python
  rotation_changes = sum(1 for i in range(1, len(orientations)) if abs(orientations[i] - orientations[i-1]) > threshold)
  rotation_frequency = rotation_changes / total_time
  ```

- **Head Movement Range**
  - **Description**: The range of head movement across different axes, showing how much the user moves their head.
  - **Calculation**: Find the maximum and minimum pitch, yaw, and roll angles during the observation period and compute the range for each axis.
  
  ```python
  pitch_range = max(pitch_angles) - min(pitch_angles)
  yaw_range = max(yaw_angles) - min(yaw_angles)
  roll_range = max(roll_angles) - min(roll_angles)
  ```

- **Head Stability**
  - **Description**: Measures the steadiness of the head position, indicating user comfort and focus.
  - **Calculation**: Compute the standard deviation of head positions over time. Lower values indicate more stability.
  
  ```python
  stability = sqrt(sum((x - mean_x)^2 + (y - mean_y)^2 + (z - mean_z)^2 for x, y, z in positions) / len(positions))
  ```

### 2. **Eye Tracking Metrics**

#### Data to Measure

- **Gaze Points**: Capture the coordinates of the user's gaze on the screen or within the environment over time.
- **Pupillary Response**: Track the diameter of the pupils over time.

#### Metrics to Calculate

- **Gaze Duration**
  - **Description**: The average time the user spends looking at the same object or area.
  - **Calculation**: Measure the time between when the gaze point hits an object or area and when it changes, then average these times.
  
  ```python
  gaze_durations = [end_time - start_time for start_time, end_time in gaze_periods]
  average_gaze_duration = sum(gaze_durations) / len(gaze_durations)
  ```

- **Gaze Change Frequency**
  - **Description**: The rate at which the gaze point changes from one object or area to another.
  - **Calculation**: Count the number of times the gaze point changes per unit of time.
  
  ```python
  gaze_changes = sum(1 for i in range(1, len(gaze_points)) if gaze_points[i] != gaze_points[i-1])
  gaze_change_frequency = gaze_changes / total_time
  ```

- **Pupillary Response**
  - **Description**: The average change in pupil size, reflecting cognitive load and light adaptation.
  - **Calculation**: Compute the difference between the maximum and minimum pupil sizes within the observed period.
  
  ```python
  pupillary_response = max(pupil_sizes) - min(pupil_sizes)
  ```

### 3. **Visual Exploration and Attention Distribution**

#### Data to Measure

- **Gaze Heatmap**: A distribution showing where the user's gaze falls across the visual field.

#### Metrics to Calculate

- **Visual Exploration Range**
  - **Description**: The spatial extent of gaze points across the visual field.
  - **Calculation**: Calculate the area covered by the gaze points in the visual field.
  
  ```python
  x_range = max(x_coords) - min(x_coords)
  y_range = max(y_coords) - min(y_coords)
  exploration_range = x_range * y_range
  ```

- **Attention Heatmap**
  - **Description**: A heatmap showing the density of gaze points over the visual field, indicating areas of high interest.
  - **Calculation**: Count gaze points within each region of the visual field and normalize by the total number of gaze points.
  
  ```python
  heatmap = [[0]*grid_width for _ in range(grid_height)]
  for x, y in gaze_points:
      heatmap[y // cell_height][x // cell_width] += 1
  normalized_heatmap = [[cell / total_gaze_points for cell in row] for row in heatmap]
  ```

### Application in Analysis

- **Pre-Adjustment vs. During-Adjustment Analysis**: Compare the metrics from the baseline (pre-adjustment) to those during the adjustment phase to understand how user behavior changes in response to their efforts to optimize comfort.
- **Long-Term Adaptation Analysis**: By comparing metrics from the baseline and post-adjustment phases, assess how well users adapt to the changes and whether their behavior stabilizes in a way that suggests improved comfort and satisfaction.

By measuring these metrics and analyzing them through the phases of environmental adjustment, you can gain insights into how design changes affect user comfort and behavior, guiding more effective adjustments and enhancements in VR environments.
## 20240407 Research Steps
> - Phase 01 VR training
> - Phase 02 VR experiment with the original settings
> - Phase 03 cognitive test in the VR environment
> - Phase 04 take off the VR headset and conduct the online survey 
> - Phase 05 (redo phase 02 - phase 04): add the visual stimuli
> - Phase 06 (redo phase 02 - phase 04): add the visual and auditory stimuli
> - Phase 07 (redo phase 02 - phase 04): add change the visual, auditory, and olfactory stimuli
> - **Physiological measurement during the virtual experience (phase 02 - phase 03)**
>

